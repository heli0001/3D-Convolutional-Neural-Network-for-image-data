import tensorflow as tf
import os
from skimage import io
import zipfile
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import SimpleITK as sitk
import glob
from keras.preprocessing.image import load_img


os.makedirs("pixelData")

# Unzip data in the newly created directory.
with zipfile.ZipFile("stroke_0.zip", "r") as z_fp:
    z_fp.extractall("./pixelData/")
with zipfile.ZipFile("stroke_1.zip", "r") as z_fp:
    z_fp.extractall("./pixelData/")
    
# Folder "stroke_0" consist of Ultrasound scans without stroke,
normal_scan_paths = [
    os.path.join(os.getcwd(), "pixelData/stroke_0", x)
    for x in os.listdir("pixelData/stroke_0")
]
# Folder "stroke_1" consist of Ultrasound scans having stroke,
abnormal_scan_paths = [
    os.path.join(os.getcwd(), "pixelData/stroke_1", x)
    for x in os.listdir("pixelData/stroke_1")
]

print("Ultrasound scans without stroke: " + str(len(normal_scan_paths)))
print("Ultrasound scans having stroke: " + str(len(abnormal_scan_paths)))


from PIL import Image
from numpy import asarray
all_abnormal_images = []
for image_path in os.listdir(abnormal_scan_paths[0]):
    image = Image.open(abnormal_scan_paths[0]+'/'+image_path)
    # convert image to numpy array
    data1 = asarray(image)
    all_abnormal_images.append(data1)
x_abnormal_train = np.array(all_abnormal_images)
print(x_abnormal_train.shape)


from PIL import Image
from numpy import asarray
def process_abnorm_scan(l):
    all_images = []
    for image_path in os.listdir(abnormal_scan_paths[l]):
        image = Image.open(abnormal_scan_paths[l]+'/'+image_path)
        # convert image to numpy array
        data1 = asarray(image)
        all_images.append(data1)
    x_train = np.array(all_images)
    return x_train

def process_norm_scan(l):
    all_images = []
    for image_path in os.listdir(normal_scan_paths[l]):
        image = Image.open(normal_scan_paths[l]+'/'+image_path)
        # convert image to numpy array
        data1 = asarray(image)
        all_images.append(data1)
    x_train = np.array(all_images)
    return x_train
    
# Read and process the scans, with slices
abnormal_scans = np.array([process_abnorm_scan(l) for l in range(len(abnormal_scan_paths))])
normal_scans = np.array([process_norm_scan(l) for l in range(len(normal_scan_paths))])

# assign 1 for stroke, for the normal ones assign 0.
abnormal_labels = np.array([1 for _ in range(len(abnormal_scans))])
normal_labels = np.array([0 for _ in range(len(normal_scans))])

x0_train, x0_test, y0_train, y0_test = train_test_split(normal_scans, normal_labels, test_size=0.2, random_state=4)
x1_train, x1_test, y1_train, y1_test = train_test_split(abnormal_scans, abnormal_labels, test_size=0.2, random_state=4)

# Split data in the ratio 80-20 for training and validation.
x_train = np.concatenate((x0_train, x1_train), axis=0)
y_train = np.concatenate((y0_train, y1_train), axis=0)
x_val = np.concatenate((x0_test, x1_test), axis=0)
y_val = np.concatenate((y0_test, y1_test), axis=0)
print(
    "Number of samples in train and validation are %d and %d."
    % (x_train.shape[0], x_val.shape[0])
)

print(x_train.shape)
print(x_val.shape)

import random
from scipy import ndimage

@tf.function
def train_preprocessing(volume, label):
    """Process training data and adding a channel."""
    ## no need to add, already have a channel 
    #volume = tf.expand_dims(volume, axis=3)
    return volume, label


def validation_preprocessing(volume, label):
    """Process validation data by only adding a channel."""
    ## no need to add, already have a channel 
    #volume = tf.expand_dims(volume, axis=3)
    return volume, label
    
# Define data loaders.
train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))
validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))

batch_size = 2
# Augment the on the fly during training.
train_dataset = (
    train_loader.shuffle(len(x_train))
    .map(train_preprocessing)
    .batch(batch_size)
    .prefetch(2)
)
# Only rescale.
validation_dataset = (
    validation_loader.shuffle(len(x_val))
    .map(validation_preprocessing)
    .batch(batch_size)
    .prefetch(2)
)


def get_model(width=170, height=170, depth=269):
    """Build a 3D convolutional neural network model."""

    inputs = keras.Input((depth,width, height,3))

    x = layers.Conv3D(filters=64, kernel_size=3, activation="relu")(inputs)
    x = layers.MaxPool3D(pool_size=2)(x)
    x = layers.BatchNormalization()(x)

    x = layers.GlobalAveragePooling3D()(x)
    x = layers.Dense(units=128, activation="relu")(x)
    x = layers.Dropout(0.3)(x)

    outputs = layers.Dense(units=1, activation="sigmoid")(x)

    # Define the model.
    model = keras.Model(inputs, outputs, name="3dcnn")
    return model

# Build model.
model = get_model(width=170, height=170, depth=269)
model.summary()

# Compile model.
initial_learning_rate = 0.0001
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
)
model.compile(
    loss="binary_crossentropy",
    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),
    metrics=["acc"],
)

# Define callbacks.
checkpoint_cb = keras.callbacks.ModelCheckpoint(
    "3d_image_classification.h5", save_best_only=True
)
early_stopping_cb = keras.callbacks.EarlyStopping(monitor="val_acc", patience=15)

# Train the model, doing validation at the end of each epoch
epochs = 100
model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=epochs,
    shuffle=True,
    verbose=2,
    callbacks=[checkpoint_cb, early_stopping_cb],
)
